<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Autodiff · NNJulia</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">NNJulia</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">NNJulia Documentation</a></li><li class="is-active"><a class="tocitem" href>Autodiff</a><ul class="internal"><li><a class="tocitem" href="#Structs-and-types"><span>Structs and types</span></a></li><li><a class="tocitem" href="#Methods-for-the-gradient"><span>Methods for the gradient</span></a></li><li><a class="tocitem" href="#Operators-between-tensors"><span>Operators between tensors</span></a></li><li><a class="tocitem" href="#Math-functions-between-tensors"><span>Math functions between tensors</span></a></li></ul></li><li><a class="tocitem" href="../dataloader/">DataLoader</a></li><li><a class="tocitem" href="../general/">General methods</a></li><li><a class="tocitem" href="../layers/">Layers</a></li><li><a class="tocitem" href="../loss/">Loss</a></li><li><a class="tocitem" href="../metrics/">Metrics</a></li><li><a class="tocitem" href="../model/">Model</a></li><li><a class="tocitem" href="../optimisers/">Optimisers</a></li><li><a class="tocitem" href="../utils/">Utils</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Autodiff</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Autodiff</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Clement-W/NNJulia.jl/blob/master/docs/src/autodiff.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Autodiff"><a class="docs-heading-anchor" href="#Autodiff">Autodiff</a><a id="Autodiff-1"></a><a class="docs-heading-anchor-permalink" href="#Autodiff" title="Permalink"></a></h1><h2 id="Structs-and-types"><a class="docs-heading-anchor" href="#Structs-and-types">Structs and types</a><a id="Structs-and-types-1"></a><a class="docs-heading-anchor-permalink" href="#Structs-and-types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.AbstractTensor" href="#NNJulia.Autodiff.AbstractTensor"><code>NNJulia.Autodiff.AbstractTensor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractTensor</code></pre><p>This type is used to counter the circular dependency between TensorDependency and Tensor.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.TensorDependency" href="#NNJulia.Autodiff.TensorDependency"><code>NNJulia.Autodiff.TensorDependency</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TensorDependency(tensorDep::AbstractTensor, gradFunction::Function)</code></pre><p>This struct represents the dependence of a tensor. This is used to keep track of the tensor&#39;s dependencies. For example, if a tensor is made up by the sum of 2 other tensor, this tensor will have 2 TensorDependency object in it&#39;s list of dependency. This struct also stores the derivative of the operation linking the dependencies, to be able to compute the gradient of the resul tensor, with respect to the  dependencies.</p><p><strong>Fields</strong></p><ul><li>tensorDep: The tensor dependence</li><li>gradFunction: This function is used to compute the gradient of the tensor that depends on TensorDep, with respect to the dependencies.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L8-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.Tensor" href="#NNJulia.Autodiff.Tensor"><code>NNJulia.Autodiff.Tensor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tensor(data::T, gradient::Union{T,Nothing}, dependencies::Union{Vector{TensorDependency},Nothing}, requires_grad::Bool) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Tensor(data::T, requires_grad::Bool=false) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Tensor(data::T, gradient::Union{T,Nothing}) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Tensor(data::T, dependencies::Union{Vector{TensorDependency},Nothing}) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>This mutable struct represents a Tensor, it is a scalar or an array that supports gradient computation</p><p><strong>Fields</strong></p><ul><li>data: The data contained in the tensor as a scalar or an array</li><li>gradient: A gradient with respect to this tensor</li><li>dependencies: A list that contains the tensors on which the current tensor depends</li><li>requires_grad: Boolean which indicates if the gradient has to be computed for this tensor</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L26-L39">source</a></section></article><h2 id="Methods-for-the-gradient"><a class="docs-heading-anchor" href="#Methods-for-the-gradient">Methods for the gradient</a><a id="Methods-for-the-gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Methods-for-the-gradient" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.backward!" href="#NNJulia.Autodiff.backward!"><code>NNJulia.Autodiff.backward!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">backward!(t::Tensor, incomingGradient::Union{T,Nothing}=nothing) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>Backpropagate a gradient through the auto differenciation graph by recurcively calling this method on the tensor dependencies. The gradient don&#39;t need to be specified if the current tensor is a scalar </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L155-L161">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.handle_broadcasting!" href="#NNJulia.Autodiff.handle_broadcasting!"><code>NNJulia.Autodiff.handle_broadcasting!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">handle_broadcasting!(t::Tensor, gradient::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>Used to support gradient computation with broadcast operations made with broadcasted operators </p><p>First, sum out the dims added by the broadcast operation, so that the gradient has the same dimensions of the tensor. To compute the gradient when a dimension is added by the broadcast operation,  the gradient is summed along the batch axis (the dimension added). This will handle this example : [1 2 ; 3 4] .+ [2,2] = [3 4; 5 6]</p><p>Then, when the operation is broadcasted but no dimension is added, the  broadcasted dims are summed by keeping the dimensions. This will handle this example : [1 2 ; 3 4] .+ [2;2] = [3 4 ; 5 6]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L195-L209">source</a></section></article><h2 id="Operators-between-tensors"><a class="docs-heading-anchor" href="#Operators-between-tensors">Operators between tensors</a><a id="Operators-between-tensors-1"></a><a class="docs-heading-anchor-permalink" href="#Operators-between-tensors" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Base.:+" href="#Base.:+"><code>Base.:+</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Base.:+(t1::Tensor, t2::Tensor)
Base.:+(t1::Tensor, notATensor::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:+(notATensor::T, t1::Tensor) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>+ operator for tensors to support addition between 2 tensors This method will add the 2 tensor&#39;s data , and then if one of the two tensors requires gradient computation, the result of t1+t2 will also requires gradient computation. Then, t1 and t2 is added in the list of dependencies of the resulting tensor, with the corresponding gradient functions.</p><ul><li>d(t1+t2)/d(t1) = 1 –&gt; multiply the incoming gradient by 1.</li><li>d(t1+t2)/d(t2) = 1, –&gt; multiply the incoming gradient by 1.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L272-L284">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:-" href="#Base.:-"><code>Base.:-</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Base.:-(t1::Tensor, t2::Tensor)
Base.:-(t1::Tensor, notATensor::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:-(notATensor::T, t1::Tensor) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:-(t2::Tensor)</code></pre><p>- operator for tensors to support substraction between 2 tensors This method will substract the 2 tensor&#39;s data, and then if one of the two tensors requires gradient computation, the result of t1-t2 will also requires gradient computation. Then, t1 and t2 is added in the list of dependencies of the resulting tensor, with the corresponding gradient functions.</p><ul><li>d(t1-t2)/d(t1) = 1 –&gt; multiply the incoming gradient by 1.</li><li>d(t1-t2)/d(t2) = -1, –&gt; multiply the incoming gradient by -1.</li><li>d(-t2)/d(t2) = -1 –&gt; multiply the incoming gradient by -1.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L347-L361">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:*" href="#Base.:*"><code>Base.:*</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Base.:*(t1::Tensor, t2::Tensor)
Base.:*(t1::Tensor, notATensor::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:*(notATensor::T, t1::Tensor) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>* operator for tensors to support multiplication and matrix multiplication between 2 tensors This method will multiply the 2 tensor&#39;s data , and then if one of the two tensors requires gradient computation, the result of t1*t2 will also requires gradient computation. Then, t1 and t2 is added in the list of dependencies of the resulting tensor, with the corresponding gradient functions.</p><p>With t1 = (n1,m1), t2 =(m1,m2) and t3 = t1 * t2 is (n1,m2) so the gradient coming from t3 is (n1,m2)</p><ul><li>d(t1*t2)/d(t1) = t2 –&gt; multiply the incoming gradient transpose(t2.data)</li><li>d(t1*t2)/d(t2) = t1, –&gt; multiply transpose(t1) by the gradient</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L441-L455">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.Broadcast.broadcasted" href="#Base.Broadcast.broadcasted"><code>Base.Broadcast.broadcasted</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Base.:broadcasted(::typeof(+), t1::Tensor, t2::Tensor)
Base.:broadcasted(::typeof(+), t1::Tensor, notATensor::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:broadcasted(::typeof(+), notATensor::T, t1::Tensor) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>Broadcast the + operator (perform element-wise addition). This works in the same way as the Base.:+ operator, but the method handle_broadcasting! is called  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L313-L320">source</a></section><section><div><pre><code class="nohighlight hljs">Base.:broadcasted(::typeof(-), t1::Tensor, t2::Tensor)
Base.:broadcasted(::typeof(-), t1::Tensor, notATensor::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:broadcasted(::typeof(-), notATensor::T, t1::Tensor) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>Broadcast the - operator (perform element-wise substraction). This works in the same way as the Base.:- operator, but the method handle_broadcasting! is called  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L406-L413">source</a></section><section><div><pre><code class="nohighlight hljs">Base.:broadcasted(::typeof(*), t1::Tensor, t2::Tensor)
Base.:broadcasted(::typeof(*), t1::Tensor, notATensor::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:broadcasted(::typeof(*), notATensor::T, t1::Tensor) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>Broadcast the * operator (perform element-wise multiplication). This works in the same way as the Base.:* operator, but the method handle_broadcasting! is called  </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L494-L501">source</a></section><section><div><pre><code class="nohighlight hljs">Base.:broadcasted(::typeof(/), t1::Tensor, t2::Tensor)
Base.:broadcasted(::typeof(/), t1::Tensor, notATensor::T) where {T&lt;:Union{AbstractArray,Float64,Int64}}
Base.:broadcasted(::typeof(/), notATensor::T, t1::Tensor) where {T&lt;:Union{AbstractArray,Float64,Int64}}</code></pre><p>Broadcast the / operator (perform element-wise multiplication) between 2 tensors.</p><ul><li>d(t1/t2)/d(t1) = 1/t2 –&gt; multiply the incoming gradient by 1/t2</li><li>d(t1/t2)/d(t2) = -t1/t2^2, –&gt; multiply the incoming gradient by -t1/t2^2</li></ul><p>Then, the method handle_broadcasting! is called on the result of the gradient computation wrt to t1 and/or t2 </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Tensor.jl#L528-L537">source</a></section></article><h2 id="Math-functions-between-tensors"><a class="docs-heading-anchor" href="#Math-functions-between-tensors">Math functions between tensors</a><a id="Math-functions-between-tensors-1"></a><a class="docs-heading-anchor-permalink" href="#Math-functions-between-tensors" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Base.sum" href="#Base.sum"><code>Base.sum</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Base.sum(t::Tensor)</code></pre><p>Return the sum of the tensor&#39;s elements.  The tensor returned requires gradient if the initial tensor requires it.</p><p>For the gradient function, incomingGradient is a one element tensor, because the output of the sum is a  scalar tensor. In the sum function, each element has the same weight  (1<em>x1 + 1</em>x2 + ... + 1*xn), so the gradient of this tensor wrt to the sum tensor is a tensor composed of ones, with the shape of the original tensor.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Functions.jl#L4-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.log" href="#Base.log"><code>Base.log</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Base.:log(t1::Tensor)</code></pre><p>Log function to perform element-wise neperian logarithm on a tensor. The tensor returned requires gradient if the initial tensor requires it.</p><ul><li>d(ln(t1))/d(t1) = 1/t1 –&gt; multiply the incoming gradient by 1/t1.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Functions.jl#L39-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.tanh" href="#Base.tanh"><code>Base.tanh</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Base.:tanh(t1::Tensor)</code></pre><p>Tanh function to perform elemnt-wise tanh on a tensor. The tensor returned requires gradient if the initial tensor requires it.</p><ul><li>d(tanh(t1))/d(t1) = (1-tanh^2(t1)) –&gt; multiply the incoming gradient by (1-tanh^2(t1))</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Functions.jl#L63-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.sigmoid" href="#NNJulia.Autodiff.sigmoid"><code>NNJulia.Autodiff.sigmoid</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sigmoid(t1::Tensor)</code></pre><p>Sigmoid function to perform elemnt-wise sigmoid on a tensor. The tensor returned requires gradient if the initial tensor requires it.</p><ul><li>d(sigmoid(t1))/d(t1) = sigmoid(t1)<em>(1-sigmoid(t1)) –&gt; multiply the incoming gradient by sigmoid(t1)</em>(1-sigmoid(t1))</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Functions.jl#L87-L94">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.relu" href="#NNJulia.Autodiff.relu"><code>NNJulia.Autodiff.relu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">relu(t1::Tensor)</code></pre><p>Relu function to perform elemnt-wise relu on a tensor. The tensor returned requires gradient if the initial tensor requires it.</p><ul><li>d(relu(t1))/d(t1) =  1 if t1&gt;0, else 0 –&gt; multiply the incoming gradient by (t1 .&gt; 0)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Functions.jl#L111-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.leakyrelu" href="#NNJulia.Autodiff.leakyrelu"><code>NNJulia.Autodiff.leakyrelu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">leakyrelu(t1::Tensor)</code></pre><p>leaky relu function to perform elemnt-wise leaky relu on a tensor. The tensor returned requires gradient if the initial tensor requires it.</p><ul><li>d(leakyrelu(t1,a))/d(t1) =  1 if t1&gt;0, else a –&gt; multiply the incoming gradient by 1 or a depending on the data</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Functions.jl#L136-L143">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NNJulia.Autodiff.softmax" href="#NNJulia.Autodiff.softmax"><code>NNJulia.Autodiff.softmax</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">softmax(t1::Tensor)</code></pre><p>Softmax function to perform softmax on a tensor. The tensor returned requires gradient if the initial tensor requires it.</p><ul><li>d(softmax(t1))/d(t1) = softmax(t1)<em>(1-softmax(t1)) –&gt; multiply the incoming gradient by softmax(t1)</em>(1-softmax(t1))</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Clement-W/NNJulia.jl/blob/f35d750c7e08b47fcdacf1b3ed889a29ec1a0fa5/src/Autodiff/Functions.jl#L165-L172">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« NNJulia Documentation</a><a class="docs-footer-nextpage" href="../dataloader/">DataLoader »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.16 on <span class="colophon-date" title="Monday 25 April 2022 22:53">Monday 25 April 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
